<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Deriving the Normal Equation</title>
  <meta name="description" content="Consider a linear modelwhereis a matrix of real numbers with ​ as the number of samples (or rows), and ​ is the number of features (or columns),is a matrix (...">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <link rel="stylesheet" type="text/css" href="http://adlersantos.com/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="http://adlersantos.com/css/print.css" media="print"> -->

  <link rel="canonical" href="http://adlersantos.com/articles/15/deriving-normal-equation">

  <link rel="alternate" type="application/rss+xml" title="Adler Santos" href="http://adlersantos.com/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
  <nav class="group">
  	<a href="http://adlersantos.com/"><img class="badge" src="http://adlersantos.com/assets/img/avatar.png" alt="CH"></a>
		
			
  	
			
		    
		      <a href="http://adlersantos.com/index.html">Writings</a>
		    
	    
  	
			
  	
			
		    
		      <a href="http://adlersantos.com/about/index.html">About</a>
		    
	    
  	
			
		    
		      <a href="http://adlersantos.com/css/print.css"></a>
		    
	    
  	
			
  	
	</nav>
</header>

    <article class="group">
      <h1>Deriving the Normal Equation</h1>
<p class="subtitle">November 22, 2015</p>

<p>Consider a linear model</p>

<div class="mathblock"><script type="math/tex; mode=display">
X\vec{\theta} = \vec{y}
</script></div>

<p>where</p>

<div class="mathblock"><script type="math/tex; mode=display">
X = \left( \begin{array}{ccc}
x_{1,1} & \dots & x_{1,n} \\
\vdots & \ddots & \vdots \\
x_{m,1} & \dots & x_{m,n} \end{array} \right)
</script></div>

<p>is a matrix of real numbers with <span>​<script type="math/tex">m</script></span> as the number of samples (or rows), and <span>​<script type="math/tex">n</script></span> is the number of features (or columns),</p>

<div class="mathblock"><script type="math/tex; mode=display">
\vec{\theta} = \left( \begin{array}{c}
\theta_1 \\
\vdots \\
\theta_m
\end{array} \right)
</script></div>

<p>is a matrix (also called a <em>vector</em>) of coefficients <span>​<script type="math/tex">\theta_i</script></span>, and</p>

<div class="mathblock"><script type="math/tex; mode=display">
\vec{y} = \left( \begin{array}{c}
y_1 \\
\vdots \\
y_m
\end{array} \right)
</script></div>

<p>is a matrix of target variables <span>​<script type="math/tex">y_i</script></span> per ith sample. <!--more--></p>

<p>The normal equation is a matrix equation that allows you to solve <span>​<script type="math/tex">\vec{\theta}</script></span> above. By doing so, you can construct a linear regression model that allows you to predict the value of the target variable for any new samples. It is given by</p>

<div class="mathblock"><script type="math/tex; mode=display">
\vec{\theta} = (X^T X)^{-1} X^T \vec{y}.
</script></div>

<p>The goal of the normal equation is to provide you with <span>​<script type="math/tex">\vec{\theta}</script></span> so that the cost function</p>

<div class="mathblock"><script type="math/tex; mode=display">
J(\vec{\theta}) = \frac{1}{2}(X\vec{\theta} - \vec{y})^T(X\vec{\theta} - \vec{y})
</script></div>

<p>is minimized.</p>

<p>You can find derivations of the normal equation on the internet, but most of them include a lot of handwaving and skip seemingly trivial, but actually tedious and crucial steps. I will derive it in this article.</p>

<h2 id="minimizing-span8203script-typemathtexjvecthetascriptspan-using-vector-calculus">Minimizing <span>​<script type="math/tex">J(\vec{\theta})</script></span> Using Vector Calculus</h2>

<p>Recall back in calculus that to minimize a function, you simply take its derivative and set it to zero. We will do the same for the cost function. But first, we need to borrow some concepts from vector calculus.</p>

<p>The derivative of a vector is called a gradient, and is denoted by the symbol <span>​<script type="math/tex">\nabla</script></span>. We subscript the symbol <span>​<script type="math/tex">\nabla</script></span> with some variable to denote that we’re taking a vector’s derivative with respect to that variable. Thus, the derivative with respect to <span>​<script type="math/tex">\theta</script></span> is simply <span>​<script type="math/tex">\nabla_\theta</script></span>.</p>

<p>Minimizing the cost function means taking its derivative with respect to <span>​<script type="math/tex">\theta</script></span> and setting the result to zero:</p>

<div class="mathblock"><script type="math/tex; mode=display">
\nabla_\theta J(\vec{\theta}) = 0.
</script></div>

<p>We will use this equation to derive the normal equation.</p>

<h2 id="some-useful-theorems">Some Useful Theorems</h2>

<p>In order to proceed with the derivation, we’re gonna need the help of some theorems. You can find these as well in any proper vector calculus textbook. Let <span>​<script type="math/tex">A</script></span>, <span>​<script type="math/tex">B</script></span>, and <span>​<script type="math/tex">C</script></span> be matrices. The two theorems we will use are the following:</p>

<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\nabla_{A^T} f(A) &= (\nabla_A f(A))^T \\
\nabla_A \text{tr}(ABA^TC) &= CAB + C^TAB^T
\end{align}
</script></div>

<p>where the <em>trace</em> <span>​<script type="math/tex">\text{tr}A</script></span> of a matrix <span>​<script type="math/tex">A</script></span> is just the sum of its diagonal elements.</p>

<p>If we set the matrix <span>​<script type="math/tex">C</script></span> as the identity matrix in the second theorem above, we simply get</p>

<div class="mathblock"><script type="math/tex; mode=display">
\nabla_A \text{tr}(ABA^T) = AB + AB^T.
</script></div>

<p>By setting <span>​<script type="math/tex">D = ABA^T</script></span>, we get</p>

<div class="mathblock"><script type="math/tex; mode=display">
\text{tr}(ABA^T) = \text{tr}(D) = f(D)
</script></div>

<p>due to the fact that the trace of a matrix is a function <span>​<script type="math/tex">f</script></span> that maps from <span>​<script type="math/tex">\mathbb{R}^{m \times n}</script></span> to <span>​<script type="math/tex">\mathbb{R}</script></span>. Taking the transpose of the last equation and noting that <span>​<script type="math/tex">(AB)^T = B^T A^T</script></span>, we get</p>

<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
[ \nabla_A f(D) &= AB + AB^T ]^T \\
(\nabla_{A} f(D))^T &= B^T A^T + BA^T
\end{align}
</script></div>

<p>But the left-hand side of the equation is just the first theorem. Thus, we have</p>

<div class="mathblock"><script type="math/tex; mode=display">
\nabla_{A^T} f(D) = B^T A^T + BA^T
</script></div>

<p>We can finally proceed with our derivation.</p>

<h2 id="all-together-now">All Together Now</h2>

<p>Let’s take the gradient of <span>​<script type="math/tex">J</script></span> with respect to <span>​<script type="math/tex">\theta</script></span></p>

<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\nabla_\theta J(\vec{\theta}) &= \nabla_\theta \left(\frac{1}{2}(X\vec{\theta} - \vec{y})^T(X\vec{\theta} - \vec{y})\right) \\
&= \frac{1}{2} \nabla_\theta \left( \vec{\theta}^T X^T X \vec{\theta} - \vec{\theta}^T X^T \vec{y} - \vec{y}^T X \vec{\theta} + \vec{y}^T \vec{y} \right)
\end{align}
</script></div>

<p>The term <span>​<script type="math/tex">\vec{y}^T \vec{y}</script></span> on the right side is a real number. This indicates that the whole term in the parentheses is some function that maps to a real number. If we use the fact that the trace of a real number is just itself, and that <span>​<script type="math/tex">\text{tr}(A) = \text{tr}(A^T)</script></span>, we get</p>

<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\nabla_\theta J(\vec{\theta}) &= \frac{1}{2} \nabla_\theta \text{tr} \left( \vec{\theta}^T X^T X \vec{\theta} - \vec{\theta}^T X^T \vec{y} - \vec{y}^T X \vec{\theta} + \vec{y}^T \vec{y} \right) \\
&= \frac{1}{2} \nabla_\theta \left[ \text{tr}(\vec{\theta}^T X^T X \vec{\theta}) - \text{tr}(\vec{y}^T X \vec{\theta}) - \text{tr}(\vec{\theta}^T X^T \vec{y}) + \text{tr}(\vec{y}^T \vec{y}) \right] \\
&= \frac{1}{2} \nabla_\theta \left[ \text{tr}(\vec{\theta}^T X^T X \vec{\theta}) - \text{tr}(\vec{y}^T X \vec{\theta})^T - \text{tr}(\vec{\theta}^T X^T \vec{y}) + \text{tr}(\vec{y}^T \vec{y}) \right] \\
&= \frac{1}{2} \nabla_\theta \left[ \text{tr}(\vec{\theta}^T X^T X \vec{\theta}) - \text{tr}(\vec{\theta}^T X^T \vec{y}) - \text{tr}(\vec{\theta}^T X^T \vec{y}) + \text{tr}(\vec{y}^T \vec{y}) \right] \\
&= \frac{1}{2} \nabla_\theta \left[ \text{tr}(\vec{\theta}^T X^T X \vec{\theta}) - 2\text{tr}(\vec{\theta}^T X^T \vec{y}) + \text{tr}(\vec{y}^T \vec{y}) \right].
\end{align}
</script></div>

<p>The term <span>​<script type="math/tex">\text{tr}(\vec{y}^T \vec{y})</script></span> has vanished because it’s treated as a constant when we’re taking the gradient with respect to <span>​<script type="math/tex">\theta</script></span>.</p>

<p>We are now left with</p>

<div class="mathblock"><script type="math/tex; mode=display">
\nabla_\theta J(\vec{\theta}) = \frac{1}{2} \nabla_\theta \left[ \text{tr}(\vec{\theta}^T X^T X \vec{\theta}) - 2\text{tr}(\vec{\theta}^T X^T \vec{y}) \right].
</script></div>

<p>If we use our derived theorem above (<span>​<script type="math/tex">\nabla_{A^T} f(D) = B^T A^T + BA^T</script></span>) and substitute as follows,</p>

<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
A^T &= \vec{\theta} \\
B &= X^TX
\end{align}
</script></div>

<p>we should get</p>

<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\nabla_\theta J(\vec{\theta}) &= \frac{1}{2} \left(X^T X \vec{\theta} + X^T X \vec{\theta} - 2 X^T \vec{y} \right) \\
&= X^T X \vec{\theta} - X^T \vec{y}
\end{align}
</script></div>

<p>Since <span>​<script type="math/tex">\nabla_\theta J(\vec{\theta}) = 0</script></span>, we finally have</p>

<div class="mathblock"><script type="math/tex; mode=display">
X^T X \vec{\theta} - X^T \vec{y} = 0
</script></div>

<p>which is the normal equation. How cool is that! :)</p>




    </article>
    <span class="print-footer">Deriving the Normal Equation - November 22, 2015 - Adler Santos</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links">
    
      <li>
        <a href="//www.twitter.com/adlersantos"><span class="icon-twitter"></span></a>
      </li>
    
  </ul>
<div class="credits">
<span>&copy; 2017 &nbsp;&nbsp;ADLER SANTOS</span></br> <br>
<span>Powered by <a href="//github.com/clayh53/tufte-jekyll">Tufte theme for Jekyll</a>.</span>
</div>
</footer>

  </body>
</html>
